{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import sys\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense, Activation\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras import callbacks\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV = False\n",
    "argvs = sys.argv\n",
    "argc = len(argvs)\n",
    "\n",
    "if argc > 1 and (argvs[1] == \"--development\" or argvs[1] == \"-d\"):\n",
    "  DEV = True\n",
    "\n",
    "if DEV:\n",
    "  epochs = 12\n",
    "else:\n",
    "  epochs = 50\n",
    "\n",
    "train_data_path = './data/train'\n",
    "validation_data_path = './data/validation'\n",
    "\n",
    "\"\"\"\n",
    "Parameters\n",
    "\"\"\"\n",
    "img_width, img_height = 150, 150\n",
    "batch_size = 64\n",
    "samples_per_epoch = 5400\n",
    "validation_steps = 2100\n",
    "nb_filters1 = 32\n",
    "nb_filters2 = 64\n",
    "conv1_size = 3\n",
    "conv2_size = 2\n",
    "pool_size = 2\n",
    "classes_num = 8\n",
    "lr = 0.0004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9311 images belonging to 8 classes.\n",
      "Found 9311 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(nb_filters1, (conv1_size, conv1_size), padding =\"same\", input_shape=(img_width, img_height, 3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "\n",
    "model.add(Conv2D(nb_filters2, (conv2_size, conv2_size), padding =\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(pool_size, pool_size), data_format='channels_first'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024)) # increased dense model from 256 to 512\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(classes_num, activation='softmax'))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=lr),\n",
    "              metrics=['accuracy'],)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "84/84 [==============================] - 481s 6s/step - loss: 2.7375 - acc: 0.3220 - val_loss: 1.5712 - val_acc: 0.4648\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - 464s 6s/step - loss: 1.6061 - acc: 0.4213 - val_loss: 1.4821 - val_acc: 0.4419\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - 466s 6s/step - loss: 1.5209 - acc: 0.4600 - val_loss: 1.3238 - val_acc: 0.5483\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - 463s 6s/step - loss: 1.4752 - acc: 0.4868 - val_loss: 1.4577 - val_acc: 0.4634\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - 464s 6s/step - loss: 1.4335 - acc: 0.4961 - val_loss: 1.2457 - val_acc: 0.5645\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - 461s 5s/step - loss: 1.3804 - acc: 0.5225 - val_loss: 1.2643 - val_acc: 0.5698\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - 479s 6s/step - loss: 1.3624 - acc: 0.5286 - val_loss: 1.1662 - val_acc: 0.5938\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - 464s 6s/step - loss: 1.3275 - acc: 0.5445 - val_loss: 1.1551 - val_acc: 0.6045\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - 494s 6s/step - loss: 1.3055 - acc: 0.5504 - val_loss: 1.1444 - val_acc: 0.5908\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - 545s 6s/step - loss: 1.2834 - acc: 0.5608 - val_loss: 1.0989 - val_acc: 0.6162\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - 455s 5s/step - loss: 1.2566 - acc: 0.5671 - val_loss: 1.1123 - val_acc: 0.6108\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - 451s 5s/step - loss: 1.2429 - acc: 0.5705 - val_loss: 1.0750 - val_acc: 0.6255\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - 452s 5s/step - loss: 1.2100 - acc: 0.5791 - val_loss: 1.0746 - val_acc: 0.6147\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - 453s 5s/step - loss: 1.2246 - acc: 0.5787 - val_loss: 1.1396 - val_acc: 0.6021\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - 456s 5s/step - loss: 1.1790 - acc: 0.5902 - val_loss: 0.9660 - val_acc: 0.6675\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - 447s 5s/step - loss: 1.1875 - acc: 0.5884 - val_loss: 1.1213 - val_acc: 0.6089\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - 454s 5s/step - loss: 1.1470 - acc: 0.6019 - val_loss: 1.0747 - val_acc: 0.6162\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - 466s 6s/step - loss: 1.1624 - acc: 0.6006 - val_loss: 1.0152 - val_acc: 0.6553\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - 449s 5s/step - loss: 1.1143 - acc: 0.6183 - val_loss: 0.9622 - val_acc: 0.6650\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - 449s 5s/step - loss: 1.1100 - acc: 0.6219 - val_loss: 1.1999 - val_acc: 0.5854\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - 463s 6s/step - loss: 1.1126 - acc: 0.6202 - val_loss: 0.9165 - val_acc: 0.6748\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - 453s 5s/step - loss: 1.0805 - acc: 0.6204 - val_loss: 0.8804 - val_acc: 0.6943\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - 497s 6s/step - loss: 1.0583 - acc: 0.6362 - val_loss: 0.9752 - val_acc: 0.6792\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - 950s 11s/step - loss: 1.0581 - acc: 0.6310 - val_loss: 0.8914 - val_acc: 0.6865\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - 905s 11s/step - loss: 1.0552 - acc: 0.6415 - val_loss: 0.9011 - val_acc: 0.6914\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - 875s 10s/step - loss: 1.0424 - acc: 0.6464 - val_loss: 0.8059 - val_acc: 0.7295\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - 866s 10s/step - loss: 1.0146 - acc: 0.6514 - val_loss: 0.8017 - val_acc: 0.7324\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - 853s 10s/step - loss: 1.0259 - acc: 0.6511 - val_loss: 0.8501 - val_acc: 0.7119\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - 877s 10s/step - loss: 1.0187 - acc: 0.6582 - val_loss: 0.7984 - val_acc: 0.7290\n",
      "Epoch 30/50\n",
      "84/84 [==============================] - 840s 10s/step - loss: 1.0043 - acc: 0.6607 - val_loss: 1.1796 - val_acc: 0.6084\n",
      "Epoch 31/50\n",
      "84/84 [==============================] - 860s 10s/step - loss: 0.9767 - acc: 0.6576 - val_loss: 0.8194 - val_acc: 0.7075\n",
      "Epoch 32/50\n",
      "84/84 [==============================] - 873s 10s/step - loss: 0.9642 - acc: 0.6654 - val_loss: 0.7739 - val_acc: 0.7383\n",
      "Epoch 33/50\n",
      "84/84 [==============================] - 880s 10s/step - loss: 0.9770 - acc: 0.6689 - val_loss: 0.7706 - val_acc: 0.7476\n",
      "Epoch 34/50\n",
      "84/84 [==============================] - 860s 10s/step - loss: 0.9493 - acc: 0.6750 - val_loss: 0.7864 - val_acc: 0.7300\n",
      "Epoch 35/50\n",
      "84/84 [==============================] - 880s 10s/step - loss: 0.9392 - acc: 0.6767 - val_loss: 0.7467 - val_acc: 0.7422\n",
      "Epoch 36/50\n",
      "84/84 [==============================] - 864s 10s/step - loss: 0.9230 - acc: 0.6853 - val_loss: 0.7185 - val_acc: 0.7695\n",
      "Epoch 37/50\n",
      "84/84 [==============================] - 884s 11s/step - loss: 0.9100 - acc: 0.6851 - val_loss: 0.6962 - val_acc: 0.7729\n",
      "Epoch 38/50\n",
      "84/84 [==============================] - 879s 10s/step - loss: 0.9255 - acc: 0.6815 - val_loss: 0.8373 - val_acc: 0.7046\n",
      "Epoch 39/50\n",
      "84/84 [==============================] - 860s 10s/step - loss: 0.9042 - acc: 0.6959 - val_loss: 0.7902 - val_acc: 0.7300\n",
      "Epoch 40/50\n",
      "84/84 [==============================] - 884s 11s/step - loss: 0.9085 - acc: 0.6934 - val_loss: 0.6558 - val_acc: 0.7891\n",
      "Epoch 41/50\n",
      "84/84 [==============================] - 884s 11s/step - loss: 0.8774 - acc: 0.6977 - val_loss: 0.6551 - val_acc: 0.7812\n",
      "Epoch 42/50\n",
      "84/84 [==============================] - 870s 10s/step - loss: 0.8750 - acc: 0.7015 - val_loss: 0.6203 - val_acc: 0.7930\n",
      "Epoch 43/50\n",
      "84/84 [==============================] - 875s 10s/step - loss: 0.8416 - acc: 0.7119 - val_loss: 0.6789 - val_acc: 0.7739\n",
      "Epoch 44/50\n",
      "84/84 [==============================] - 921s 11s/step - loss: 0.8549 - acc: 0.7076 - val_loss: 0.6157 - val_acc: 0.8091\n",
      "Epoch 45/50\n",
      "84/84 [==============================] - 927s 11s/step - loss: 0.8652 - acc: 0.7127 - val_loss: 0.7582 - val_acc: 0.7632\n",
      "Epoch 46/50\n",
      "84/84 [==============================] - 867s 10s/step - loss: 0.8431 - acc: 0.7102 - val_loss: 0.6517 - val_acc: 0.7812\n",
      "Epoch 47/50\n",
      "84/84 [==============================] - 874s 10s/step - loss: 0.8533 - acc: 0.7160 - val_loss: 0.6294 - val_acc: 0.7964\n",
      "Epoch 48/50\n",
      "84/84 [==============================] - 880s 10s/step - loss: 0.8378 - acc: 0.7211 - val_loss: 0.6790 - val_acc: 0.8008\n",
      "Epoch 49/50\n",
      "84/84 [==============================] - 871s 10s/step - loss: 0.8180 - acc: 0.7260 - val_loss: 0.5361 - val_acc: 0.8154\n",
      "Epoch 50/50\n",
      "84/84 [==============================] - 882s 10s/step - loss: 0.7963 - acc: 0.7317 - val_loss: 0.5788 - val_acc: 0.8271\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=samples_per_epoch // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps // batch_size)\n",
    "\n",
    "target_dir = './models/'\n",
    "if not os.path.exists(target_dir):\n",
    "  os.mkdir(target_dir)\n",
    "model.save('./models/modelv4.0_8.categories.h5')\n",
    "model.save_weights('./models/weights_v4.0_8.categories.0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict_multiclass\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras.models import Sequential, load_model\n",
    "\n",
    "\n",
    "img_width, img_height = 150, 150\n",
    "model_path = './models/modelv4.0_8.categories.h5'\n",
    "model_weights_path = './models/weights_v4.0_8.categories.0.h5'\n",
    "model = load_model(model_path)\n",
    "model.load_weights(model_weights_path)\n",
    "\n",
    "def predict(file):\n",
    "  x = load_img(file, target_size=(img_width,img_height))\n",
    "  x = img_to_array(x)\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "  array = model.predict(x)\n",
    "  result = array[0]\n",
    "  answer = np.argmax(result)\n",
    "  if answer == 0:\n",
    "    print(\"Label: banana\")\n",
    "  elif answer == 1:\n",
    "    print(\"Labels: battery\")\n",
    "  elif answer == 2:\n",
    "    print(\"Label: computer\")\n",
    "  elif answer == 3:\n",
    "    print(\"Label: glass_bottel\")\n",
    "  elif answer == 4:\n",
    "    print(\"Label: light_blub\")\n",
    "  elif answer == 5:\n",
    "    print(\"Label: paper\")\n",
    "  elif answer == 6:\n",
    "    print(\"Label: phone\")\n",
    "  elif answer == 7:\n",
    "    print(\"Label: plastic\")\n",
    "  \n",
    "\n",
    "  return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: phone\n"
     ]
    }
   ],
   "source": [
    "# classify a picture\n",
    "for i, ret in enumerate(os.walk('./test-data/whatishere')):\n",
    "  for i, filename in enumerate(ret[2]):\n",
    "    result = predict(ret[0] + '/' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
